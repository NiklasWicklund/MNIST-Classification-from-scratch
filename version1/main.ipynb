{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST - Handwritten Digit Recognition, Deep Neural Network from scratch\n",
    "\n",
    "**Author:** Niklas Wicklund\n",
    "\n",
    "**Description:** This notebook contains an implementation of a deep neural network from scratch used for classifying handwritten digits from the MNIST dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import mnist\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxiliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    eps = 1e-12\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "def ReLu(X):\n",
    "    return np.maximum(0, X)\n",
    "\n",
    "def one_hot_encoding(y, num_classes=10):\n",
    "    one_hot = np.zeros((y.shape[0], num_classes))\n",
    "    one_hot[np.arange(y.shape[0]), y] = 1\n",
    "    return one_hot.T\n",
    "\n",
    "def loss(Y, P):\n",
    "    \"\"\"\n",
    "\tComputes the cross entropy loss \n",
    "\tY: one-hot encoded labels\n",
    "\tprobablities: activation probabilities\n",
    "    loss_function: \"cross_entropy\" or \"mbce\"\n",
    "\t\"\"\"\n",
    "    eps = 1e-12\n",
    "    loss = np.sum(-Y*np.log(P + eps))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining our neural network class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    # define the constructor with proper python docstring\n",
    "    def __init__(self, input_nodes, hidden_nodes, output_nodes, learning_rate = 0.1):\n",
    "        '''\n",
    "        Arguments:\n",
    "            input_nodes: number of input nodes\n",
    "            hidden_nodes: each hidden layer and its #nodes are passed as a list\n",
    "            output_nodes: number of output nodes\n",
    "            learning_rate: learning rate\n",
    "        '''\n",
    "        self.input_nodes = input_nodes\n",
    "        self.hidden_nodes = hidden_nodes\n",
    "        self.output_nodes = output_nodes\n",
    "        self.lr = learning_rate\n",
    "        self.intialize_weights_bias()\n",
    "    def intialize_weights_bias(self):\n",
    "        '''\n",
    "        Initialize the weights and biases of the neural network\n",
    "        '''\n",
    "        self.W = []\n",
    "        self.b = []\n",
    "        # add all nodes to allow for easy iteration\n",
    "        nodes = [self.input_nodes] + self.hidden_nodes + [self.output_nodes]\n",
    "        for i in range(len(nodes)-1):\n",
    "            self.W.append(np.random.normal(0,0.01,(nodes[i+1], nodes[i]))) # dim should be rows x cols (output x input)\n",
    "            self.b.append(np.zeros((nodes[i+1],1))) # dim should be rows x 1 (output x 1)\n",
    "\n",
    "    def compute_accuracy(self,X, y):\n",
    "        # Get our probabilites (returns P with dimensions (K x n))\n",
    "        P,_ = self.forward_pass(X)\n",
    "        arg_maxes = P.argmax(0)\n",
    "        avg = np.mean(arg_maxes)\n",
    "        return np.sum((arg_maxes == y))/arg_maxes.shape[0]\n",
    "    \n",
    "    def forward_pass(self, X):\n",
    "        layer_input = X\n",
    "        # The amount of intermediate results are equal to the amount of hidden layers.\n",
    "        Hs = []\n",
    "        # len(W) = 3; run one time i = 0,1\n",
    "        for i in range(len(self.W)-1):\n",
    "            #print('i: ', i, 'W shape: ', self.W[i].shape, 'X shape: ', X.shape, 'b shape: ', self.b[i].shape)\n",
    "            layer_input = np.maximum(self.W[i]@layer_input + self.b[i],0)\n",
    "            Hs.append(layer_input)\n",
    "\n",
    "        S = self.W[-1]@layer_input + self.b[-1]\n",
    "        P = softmax(S)\n",
    "        return P, Hs\n",
    "    \n",
    "    def backward_pass(self, X, Y, P,Hs):\n",
    "        n = X.shape[1] # number of samples\n",
    "        dWs = [None] * len(self.W)\n",
    "        dbs = [None] * len(self.b)\n",
    "        G = -(Y-P)\n",
    "        for i in range(len(self.W)-1, 0, -1):\n",
    "        \n",
    "            dWs[i] = (1/n)*G@Hs[i-1].T\n",
    "            dbs[i] = (1/n) * (G@np.ones(shape=(n,1)))\n",
    "\n",
    "            G = self.W[i].T@G\n",
    "            G = G * (Hs[i-1] > 0)\n",
    "        dWs[0] = (1/n)*G@X.T\n",
    "        dbs[0] = (1/n) * (G@np.ones(shape=(n,1)))\n",
    "        \n",
    "\n",
    "        return dWs, dbs\n",
    "    \n",
    "    def update_weights_bias(self, dWs, dbs):\n",
    "        '''\n",
    "        Arguments:\n",
    "            dWs: list of gradients of weights\n",
    "            dbs: list of gradients of biases\n",
    "        '''\n",
    "        for i in range(len(self.W)):\n",
    "            self.W[i] -= self.lr * dWs[i]\n",
    "            self.b[i] -= self.lr * dbs[i]\n",
    "\n",
    "    def train(self,X,Y,y,settings):\n",
    "        '''\n",
    "        Arguments:\n",
    "            X: input data\n",
    "            Y: true one hot encoded labels\n",
    "            y: true labels\n",
    "            settings: dictionary of hyperparameters\n",
    "        '''\n",
    "        epochs = settings['epochs']\n",
    "        self.lr = settings['learning_rate'] if 'learning_rate' in settings else self.lr\n",
    "        \n",
    "        for i in range(epochs):\n",
    "            P, Hs = self.forward_pass(X)\n",
    "            dWs, dbs = self.backward_pass(X,Y,P,Hs)\n",
    "            self.update_weights_bias(dWs, dbs)\n",
    "            \n",
    "            # Log results\n",
    "            if i % 50 == 0:\n",
    "                accuracy = self.compute_accuracy(X,y)\n",
    "                print(f'Accuracy at epoch {i}: {accuracy}')\n",
    "        return self.W, self.b\n",
    "\n",
    "        \n",
    "            \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the dataset\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "\n",
    "\n",
    "# Reshape the data\n",
    "X_train = X_train.reshape(60000, 784)\n",
    "X_test = X_test.reshape(10000, 784)\n",
    "#Want input as a column vector\n",
    "X_train = X_train.T\n",
    "X_test = X_test.T\n",
    "\n",
    "#Normalize the data\n",
    "X_train = X_train/255\n",
    "X_test = X_test/255\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 10000)\n",
      "(784, 10000)\n",
      "(10000,)\n",
      "Accuracy at epoch 0: 0.1998\n",
      "Accuracy at epoch 50: 0.4425\n",
      "Accuracy at epoch 100: 0.7259\n",
      "Accuracy at epoch 150: 0.832\n",
      "Accuracy at epoch 200: 0.8681\n",
      "Accuracy at epoch 250: 0.8852\n",
      "Accuracy at epoch 300: 0.8947\n",
      "Accuracy at epoch 350: 0.8992\n",
      "Accuracy at epoch 400: 0.9055\n",
      "Accuracy at epoch 450: 0.9091\n",
      "Accuracy at epoch 500: 0.9133\n",
      "Accuracy at epoch 550: 0.917\n",
      "Accuracy at epoch 600: 0.9191\n",
      "Accuracy at epoch 650: 0.921\n",
      "Accuracy at epoch 700: 0.9226\n",
      "Accuracy at epoch 750: 0.9238\n",
      "Accuracy at epoch 800: 0.9275\n",
      "Accuracy at epoch 850: 0.9281\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork(784, [50], 10)\n",
    "\n",
    "settings = {\n",
    "    'epochs': 900,\n",
    "    'learning_rate': 0.1\n",
    "}\n",
    "\n",
    "X = X_train[:, 0:10000] # first 1000 samples\n",
    "y = y_train[0:10000] # first 1000 samples\n",
    "Y = one_hot_encoding(y)\n",
    "print(Y.shape)\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "nn.train(X,Y, y, settings);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculate accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set:  0.9122\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy\n",
    "print('Accuracy on test set: ', nn.compute_accuracy(X_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
